---
description: Background jobs package - Trigger.dev v3 tasks for notifications and processing
globs: ["packages/jobs/**/*"]
alwaysApply: false
---
# Jobs Package Implementation Rules

## Overview

The jobs package contains all Trigger.dev v3 background job definitions shared across applications. It provides reusable jobs for notifications, data processing, and scheduled tasks.

## Core Technologies

- **Framework**: Trigger.dev v3
- **Runtime**: Node.js (deployed to Trigger.dev cloud)
- **Database**: Supabase via service role

## Project Structure

```
packages/jobs/
├── src/
│   ├── notifications/          # Notification jobs
│   │   ├── execution-error.ts
│   │   └── high-consumption-notifier.ts
│   ├── index.ts               # Main exports
│   └── types.ts               # Shared types
├── package.json
└── tsconfig.json
```

## Trigger.dev v3 Setup

### Package Configuration

```json
// package.json
{
  "name": "@proxed/jobs",
  "exports": {
    ".": "./src/index.ts"
  },
  "dependencies": {
    "@trigger.dev/sdk": "^3.x",
    "@proxed/supabase": "workspace:*",
    "@proxed/mail": "workspace:*",
    "@proxed/utils": "workspace:*"
  }
}
```

## Job Implementation Patterns

### Basic Job Structure

```typescript
// notifications/execution-error.ts
import { task } from "@trigger.dev/sdk/v3";
import { createJobClient } from "@proxed/supabase/job";
import { Mailer, ResendProvider } from "@proxed/mail";

export const notifyExecutionError = task({
  id: "notify-execution-error",
  retry: {
    maxAttempts: 3,
    factor: 2,
    minTimeoutInMs: 1000,
    maxTimeoutInMs: 10000,
  },
  run: async (payload: {
    executionId: string;
    projectId: string;
    error: {
      message: string;
      code?: string;
      timestamp: string;
    };
  }) => {
    const supabase = createJobClient();

    // Get project and team details
    const { data: execution } = await supabase
      .from("executions")
      .select(`
        *,
        project:projects!inner(
          id,
          name,
          team:teams!inner(
            id,
            name,
            notification_email
          )
        )
      `)
      .eq("id", payload.executionId)
      .single();

    if (!execution) {
      throw new Error("Execution not found");
    }

    // Check notification preferences
    const { data: preferences } = await supabase
      .from("team_notification_preferences")
      .select("*")
      .eq("team_id", execution.project.team.id)
      .single();

    if (!preferences?.execution_errors) {
      console.log("Execution error notifications disabled for team");
      return { notified: false };
    }

    // Send email notification
    const mailer = new Mailer(
      new ResendProvider(process.env.RESEND_API_KEY!)
    );

    await mailer.send({
      to: execution.project.team.notification_email,
      template: "execution-error",
      data: {
        projectName: execution.project.name,
        errorMessage: payload.error.message,
        executionId: payload.executionId,
        dashboardUrl: `${process.env.APP_URL}/projects/${payload.projectId}/executions/${payload.executionId}`,
      },
    });

    // Log notification
    await supabase.from("notification_logs").insert({
      team_id: execution.project.team.id,
      type: "execution_error",
      recipient: execution.project.team.notification_email,
      metadata: {
        execution_id: payload.executionId,
        project_id: payload.projectId,
        error: payload.error,
      },
    });

    return { notified: true };
  },
});
```

### Scheduled Job Pattern

```typescript
// notifications/high-consumption-notifier.ts
import { schedules } from "@trigger.dev/sdk/v3";
import { createJobClient } from "@proxed/supabase/job";
import { checkUsageLimits } from "@proxed/utils";

export const highConsumptionNotifier = schedules.task({
  id: "high-consumption-notifier",
  // Run every hour
  cron: "0 * * * *",
  run: async (payload) => {
    const supabase = createJobClient();

    // Get all teams with notification preferences
    const { data: teams } = await supabase
      .from("teams")
      .select(`
        *,
        subscription:subscriptions!inner(
          plan,
          current_period_start,
          current_period_end
        ),
        notification_preferences:team_notification_preferences!inner(
          high_consumption_threshold,
          high_consumption_enabled
        )
      `)
      .eq("notification_preferences.high_consumption_enabled", true);

    if (!teams?.length) {
      return { processed: 0 };
    }

    const notifications = [];

    for (const team of teams) {
      // Get current period usage
      const { data: usage } = await supabase
        .rpc("get_team_usage", {
          team_id: team.id,
          start_date: team.subscription.current_period_start,
          end_date: new Date().toISOString(),
        });

      if (!usage) continue;

      // Check if usage exceeds threshold
      const limits = checkUsageLimits(team.subscription.plan, {
        requests: usage.total_requests,
        projects: usage.total_projects,
      });

      const threshold = team.notification_preferences.high_consumption_threshold;
      const exceededThreshold = limits.limits.some(
        limit => limit.percentage >= threshold
      );

      if (exceededThreshold) {
        // Check if we already sent notification in last 24 hours
        const { data: recentNotification } = await supabase
          .from("notification_logs")
          .select("id")
          .eq("team_id", team.id)
          .eq("type", "high_consumption")
          .gte("created_at", new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString())
          .limit(1);

        if (!recentNotification?.length) {
          notifications.push({
            teamId: team.id,
            limits: limits.limits,
            threshold,
          });
        }
      }
    }

    // Send notifications in parallel
    await Promise.all(
      notifications.map(notification =>
        sendHighConsumptionNotification.trigger(notification)
      )
    );

    return {
      processed: teams.length,
      notified: notifications.length,
    };
  },
});

export const sendHighConsumptionNotification = task({
  id: "send-high-consumption-notification",
  run: async (payload: {
    teamId: string;
    limits: Array<{
      resource: string;
      used: number;
      limit: number;
      percentage: number;
    }>;
    threshold: number;
  }) => {
    // Implementation similar to execution error notification
  },
});
```

### Batch Processing Job

```typescript
// data-processing/aggregate-stats.ts
import { task } from "@trigger.dev/sdk/v3";
import { createJobClient } from "@proxed/supabase/job";

export const aggregateProjectStats = task({
  id: "aggregate-project-stats",
  machine: {
    preset: "medium-1x", // More resources for heavy processing
  },
  run: async (payload: {
    projectId: string;
    date: string; // YYYY-MM-DD
  }) => {
    const supabase = createJobClient();

    // Get all executions for the day
    const startOfDay = new Date(`${payload.date}T00:00:00Z`);
    const endOfDay = new Date(`${payload.date}T23:59:59Z`);

    const { data: executions } = await supabase
      .from("executions")
      .select("*")
      .eq("project_id", payload.projectId)
      .gte("created_at", startOfDay.toISOString())
      .lte("created_at", endOfDay.toISOString());

    if (!executions?.length) {
      return { processed: 0 };
    }

    // Calculate aggregates
    const stats = {
      total_requests: executions.length,
      successful_requests: executions.filter(e => e.status === "success").length,
      failed_requests: executions.filter(e => e.status === "error").length,
      total_tokens: executions.reduce((sum, e) => sum + (e.total_tokens || 0), 0),
      total_cost: executions.reduce((sum, e) => sum + (e.cost || 0), 0),
      avg_latency: executions.reduce((sum, e) => sum + (e.latency || 0), 0) / executions.length,
      models_used: [...new Set(executions.map(e => e.model))],
    };

    // Upsert daily stats
    await supabase
      .from("project_daily_stats")
      .upsert({
        project_id: payload.projectId,
        date: payload.date,
        ...stats,
        updated_at: new Date().toISOString(),
      });

    return { processed: executions.length, stats };
  },
});

// Scheduled job to trigger daily aggregation
export const dailyStatsAggregator = schedules.task({
  id: "daily-stats-aggregator",
  cron: "0 2 * * *", // Run at 2 AM UTC every day
  run: async (payload) => {
    const supabase = createJobClient();
    const yesterday = new Date(payload.timestamp);
    yesterday.setDate(yesterday.getDate() - 1);
    const date = yesterday.toISOString().split("T")[0];

    // Get all active projects
    const { data: projects } = await supabase
      .from("projects")
      .select("id")
      .eq("status", "active");

    if (!projects?.length) {
      return { processed: 0 };
    }

    // Trigger aggregation for each project
    const jobs = await Promise.all(
      projects.map(project =>
        aggregateProjectStats.trigger({
          projectId: project.id,
          date,
        })
      )
    );

    return {
      date,
      projects: projects.length,
      jobs: jobs.map(j => j.id),
    };
  },
});
```

### Subtask Pattern

```typescript
// notifications/team-digest.ts
import { task } from "@trigger.dev/sdk/v3";

export const sendTeamDigest = task({
  id: "send-team-digest",
  run: async (payload: { teamId: string; period: "weekly" | "monthly" }) => {
    const supabase = createJobClient();

    // Get team members
    const { data: members } = await supabase
      .from("team_members")
      .select("*, user:users(*)")
      .eq("team_id", payload.teamId)
      .eq("receive_digest", true);

    if (!members?.length) {
      return { sent: 0 };
    }

    // Get digest data
    const digestData = await getTeamDigestData(payload.teamId, payload.period);

    // Send individual emails in parallel (subtasks)
    const emailJobs = await Promise.all(
      members.map(member =>
        sendDigestEmail.trigger({
          email: member.user.email,
          name: member.user.full_name,
          ...digestData,
        })
      )
    );

    return {
      sent: emailJobs.length,
      jobs: emailJobs.map(j => j.id),
    };
  },
});

export const sendDigestEmail = task({
  id: "send-digest-email",
  retry: {
    maxAttempts: 3,
  },
  run: async (payload: {
    email: string;
    name: string;
    teamName: string;
    stats: any;
    highlights: any[];
  }) => {
    const mailer = new Mailer(
      new ResendProvider(process.env.RESEND_API_KEY!)
    );

    await mailer.send({
      to: payload.email,
      template: "team-digest",
      data: payload,
    });

    return { sent: true };
  },
});
```

## Error Handling

```typescript
// Comprehensive error handling
export const robustJob = task({
  id: "robust-job",
  retry: {
    maxAttempts: 5,
    factor: 2,
    minTimeoutInMs: 1000,
    maxTimeoutInMs: 30000,
  },
  run: async (payload: any) => {
    try {
      // Main logic
      return { success: true };
    } catch (error) {
      // Log error details
      console.error("Job failed:", {
        error: error instanceof Error ? error.message : "Unknown error",
        payload,
        timestamp: new Date().toISOString(),
      });

      // Determine if error is retryable
      if (error instanceof Error) {
        if (error.message.includes("rate limit")) {
          // Wait longer for rate limits
          await new Promise(resolve => setTimeout(resolve, 60000));
          throw error; // Will retry
        }

        if (error.message.includes("not found")) {
          // Don't retry for not found errors
          return { success: false, error: "Resource not found" };
        }
      }

      // Re-throw for retry
      throw error;
    }
  },
  handleError: async (error, { ctx }) => {
    // Final error handling after all retries
    await notifyAdminOfJobFailure({
      jobId: ctx.run.id,
      error: error.message,
      attempts: ctx.run.attempts,
    });
  },
});
```

## Type Safety

```typescript
// types.ts
export interface JobContext {
  jobId: string;
  attempt: number;
  timestamp: Date;
}

export interface NotificationPayload {
  recipientId: string;
  type: "email" | "sms" | "push";
  template: string;
  data: Record<string, any>;
}

// Type-safe job definition
export const typedJob = task({
  id: "typed-job",
  run: async (payload: NotificationPayload, { ctx }) => {
    // Implementation with full type safety
  },
});
```

## Testing Jobs

```typescript
// __tests__/jobs.test.ts
import { describe, it, expect, mock } from "bun:test";
import { notifyExecutionError } from "../notifications/execution-error";

describe("Notification Jobs", () => {
  it("should send execution error notification", async () => {
    const mockSupabase = {
      from: () => ({
        select: () => ({
          eq: () => ({
            single: async () => ({
              data: {
                project: {
                  name: "Test Project",
                  team: {
                    notification_email: "team@example.com",
                  },
                },
              },
            }),
          }),
        }),
      }),
    };

    // Mock dependencies
    mock.module("@proxed/supabase/job", () => ({
      createJobClient: () => mockSupabase,
    }));

    // Test job execution
    const result = await notifyExecutionError.run({
      executionId: "test-123",
      projectId: "project-123",
      error: {
        message: "Test error",
        timestamp: new Date().toISOString(),
      },
    });

    expect(result.notified).toBe(true);
  });
});
```

## Best Practices

1. **Always use unique job IDs across the project**
2. **Configure appropriate retry strategies**
3. **Use subtasks for parallel processing**
4. **Log important events for debugging**
5. **Handle errors gracefully with fallbacks**
6. **Set machine presets for resource-intensive jobs**

## Common Pitfalls

❌ Don't use Trigger.dev v2 syntax (client.defineJob)
❌ Don't forget to export all tasks
❌ Don't create circular dependencies between jobs
❌ Don't perform heavy computations without proper machine config
❌ Don't skip error handling and retries
